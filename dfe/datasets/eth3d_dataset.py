"""Colmap dataset for fundametal matrix estimation. Derived from FundamentalMatrixDataset.
"""

import numpy as np
import os
import cv2
from tqdm import tqdm
import collections

from dfe.datasets import FundamentalMatrixDataset
from dfe.utils import colmap_read, colmap_utils


class Eth3dDataset(FundamentalMatrixDataset):
    """Colmap dataset for fundametal matrix estimation. Derived from FundamentalMatrixDataset.
    """

    def __init__(
        self,
        path,
        num_points=-1,
        threshold=1,
        max_F=None,
        random=False,
        min_matches=20,
        compute_virtual_points=True,
        visualize=False,
        vis_output_dir=None,
        mode="test"  # Added mode parameter, default is "test"
    ):
        """Init.

        Args:
            path (str): path to dataset folder
            num_points (int, optional): number of points per sample. Defaults to -1.
            threshold (int, optional): epipolar threshold. Defaults to 1.
            max_F (int, optional): maximal number of samples (if None: use all). Defaults to None.
            random (bool, optional): random database access. Defaults to False.
            min_matches (int, optional): minimal number of good matches per sample. Defaults to 20.
            compute_virtual_points (bool, optional): whether to compute virtual points. Defaults to True.
            visualize (bool, optional): whether to visualize matches. Defaults to True.
            vis_output_dir (str, optional): directory to save visualizations. If None, uses 'vis' under dataset path. Defaults to None.
            mode (str, optional): "train" or "test" mode. Defaults to "test".
        """
        super(Eth3dDataset, self).__init__(num_points)
        # Always create a visualization output directory under the dataset path
        if vis_output_dir is None:
            vis_output_dir = os.path.join(path, "vis")
        
        # Always ensure visualization directory exists
        os.makedirs(vis_output_dir, exist_ok=True)
    
        self.mode = mode
        self.compute_virtual_points = compute_virtual_points
        
        cameras = colmap_read.read_cameras_text("%s/dslr_calibration_undistorted/cameras.txt" % path)
        images = colmap_read.read_images_text("%s/dslr_calibration_undistorted/images.txt" % path)

        # Path to feature directory generated by gen_sideinfo.py
        feature_dir = os.path.join(path, "dslr_calibration_undistorted", "feature")
        
        self.img_paths = []
        self.pts = []
        self.R_gt = []  # Ground truth rotation matrix
        self.t_gt = []  # Ground truth translation vector (normalized)
        self.size_1 = []
        self.size_2 = []
        self.intrinsics = []

        # Initialize FLANN matcher
        FLANN_INDEX_KDTREE = 1
        index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)
        search_params = dict(checks=50)
        flann = cv2.FlannBasedMatcher(index_params, search_params)

        # Traverse all image pairs
        img_ids = list(images.keys())
        min_covisible_points = 100  # Minimum number of covisible points
        min_translation_norm = 0.075  # Minimum translation norm threshold
        
        processed_pairs = 0
        max_pairs = max_F if max_F else float('inf')
        
        # Create visualization output directory if needed
        if visualize and vis_output_dir is not None:
            os.makedirs(vis_output_dir, exist_ok=True)
        
        print(f"Processing image pairs for matching...")
        for i in tqdm(range(len(img_ids))):
            if processed_pairs >= max_pairs:
                break
                
            img1_id = img_ids[i]
            img1 = images[img1_id]
            img1_filename = os.path.basename(img1.name)
            img1_feature_path = os.path.join(feature_dir, img1_filename.replace('.JPG', '.txt'))
            
            # Skip if feature file doesn't exist
            if not os.path.exists(img1_feature_path):
                continue
                
            # Load features from img1
            img1_keypoints, img1_descriptors, img1_scales, img1_orientations = self._load_features(img1_feature_path)
            
            for j in range(i+1, len(img_ids)):
                if processed_pairs >= max_pairs:
                    break
                    
                img2_id = img_ids[j]
                img2 = images[img2_id]
                img2_filename = os.path.basename(img2.name)
                img2_feature_path = os.path.join(feature_dir, img2_filename.replace('.JPG', '.txt'))
                
                # Skip if feature file doesn't exist
                if not os.path.exists(img2_feature_path):
                    continue
                
                # Find common 3D points between the two images to check baseline constraint
                common_points = set(img1.xys.keys()).intersection(set(img2.xys.keys()))
                
                # Skip if not enough covisible points
                if len(common_points) < min_covisible_points:
                    continue
                
                # Calculate relative pose to check translation constraint
                R1 = img1.qvec2rotmat()
                R2 = img2.qvec2rotmat()
                t1 = img1.tvec
                t2 = img2.tvec
                R_rel = np.dot(R1.T, R2)
                t_rel = np.dot(R1.T, t2 - t1)
                
                # Skip pairs with small translation
                if np.linalg.norm(t_rel) < min_translation_norm:
                    continue
                
                # Normalize t_rel to unit vector
                t_rel_norm = t_rel / np.linalg.norm(t_rel)
                
                # Get camera intrinsics
                K1 = colmap_utils.get_cam(img1, cameras)
                
                # Load features from img2
                img2_keypoints, img2_descriptors, img2_scales, img2_orientations = self._load_features(img2_feature_path)
                
                # Match descriptors using FLANN
                img1_descriptors = img1_descriptors.astype(np.float32)
                img2_descriptors = img2_descriptors.astype(np.float32)
                matches = flann.knnMatch(img1_descriptors, img2_descriptors, k=2)
                
                # Apply ratio test
                good_matches = []
                for m, n in matches:
                    if m.distance < 0.8 * n.distance:  # Ratio test
                        good_matches.append(m)
                
                # Filter to ensure one-to-one correspondence
                good_matches = self._filter_one_to_one_matches(good_matches)
                
                # Skip if not enough good matches
                if len(good_matches) < min_matches:
                    continue
                
                # Apply RANSAC to filter outliers
                src_pts = np.float32([img1_keypoints[m.queryIdx] for m in good_matches])
                dst_pts = np.float32([img2_keypoints[m.trainIdx] for m in good_matches])
                
                # Use RANSAC to estimate the fundamental matrix and find inliers
                F, mask = cv2.findFundamentalMat(src_pts, dst_pts, cv2.FM_RANSAC, 
                                                ransacReprojThreshold=3.0, confidence=0.99)
                
                # Check if F estimation was successful
                if F is None or F.shape != (3, 3):
                    continue
                
                # Convert mask to boolean array and filter matches
                if mask is None:
                    continue
                    
                mask = mask.ravel().astype(bool)
                ransac_matches = [good_matches[i] for i in range(len(good_matches)) if mask[i]]
                
                # Skip if not enough inliers
                if len(ransac_matches) < min_matches:
                    continue
                print(len(ransac_matches))
                
                # Create pairs with side information, now using only RANSAC inliers
                pairs = []
                for match in ransac_matches:
                    idx1, idx2 = match.queryIdx, match.trainIdx
                    
                    # Extract keypoint coordinates
                    x1, y1 = img1_keypoints[idx1]
                    x2, y2 = img2_keypoints[idx2]
                    
                    # Extract descriptor distance
                    descriptor_1 = img1_descriptors[idx1]
                    descriptor_2 = img2_descriptors[idx2]
                    dist = np.sqrt(np.mean((descriptor_1 - descriptor_2) ** 2))
                    
                    # Extract relative scale
                    scale1 = img1_scales[idx1]
                    scale2 = img2_scales[idx2]
                    rel_scale = np.abs(scale1 - scale2)
                    
                    # Extract relative orientation
                    angle1 = img1_orientations[idx1]
                    angle2 = img2_orientations[idx2]
                    rel_orient = np.minimum(np.abs(angle1 - angle2), np.abs(360 - np.abs(angle1 - angle2)))
                    
                    # Create pair with side information
                    pair = [x1, y1, x2, y2, dist, rel_scale, rel_orient]
                    pairs.append(pair)
                
                pairs = np.array(pairs)
                
                # Proceed only if we have enough matches
                if len(pairs) >= min_matches:
                    self.pts.append(pairs)
                    self.R_gt.append(R_rel)
                    self.t_gt.append(t_rel_norm)
                    self.intrinsics.append(K1)
                    
                    img1_path = "%s/%s" % (path, img1.name)
                    img2_path = "%s/%s" % (path, img2.name)
                    self.img_paths.append((img1_path, img2_path))
                    
                    processed_pairs += 1
                    
                    # Draw image pair with correspondences connected using different color of lines
                    if visualize:
                        img_pair_name = f"match_{processed_pairs:04d}_{os.path.basename(img1_path)}_{os.path.basename(img2_path)}"
                        output_path = os.path.join(vis_output_dir if vis_output_dir else '.', img_pair_name)
                        self._draw_matches(img1_path, img2_path, pairs[:, :4], output_path)
                    
        
        print(f"Created {len(self.pts)} pairs for training/evaluation")
        
        # Precompute F_gt and virtual points for training mode
        if mode == "train" and compute_virtual_points:
            self.F_gt = []
            self.pts1_virt = []
            self.pts2_virt = []
            
            for i in range(len(self.pts)):
                # Calculate fundamental matrix from R, t, and K
                R = self.R_gt[i]
                t = self.t_gt[i]
                K_mat = self.intrinsics[i]
                
                # Calculate essential matrix
                t_cross = np.array([
                    [0, -t[2], t[1]],
                    [t[2], 0, -t[0]],
                    [-t[1], t[0], 0]
                ])
                E = t_cross @ R
                
                # Calculate fundamental matrix
                F = np.linalg.inv(K_mat).T @ E @ np.linalg.inv(K_mat)
                F = F / np.linalg.norm(F)  # Normalize F
                self.F_gt.append(F)
                
                # Generate virtual points for training
                if compute_virtual_points:
                    # Try to load sample image to get dimensions
                    try:
                        img1_path, _ = self.img_paths[i]
                        img = cv2.imread(img1_path)
                        if img is not None:
                            h, w = img.shape[:2]
                        else:
                            h, w = 2000, 3000  # Default size for ETH3D
                    except:
                        h, w = 2000, 3000  # Default size if image loading fails
                    
                    pts1_virt = np.zeros((100, 3))
                    pts2_virt = np.zeros((100, 3))
                    
                    # Create grid of points
                    for j in range(10):
                        for k in range(10):
                            idx = j * 10 + k
                            x = w * (k + 0.5) / 10
                            y = h * (j + 0.5) / 10
                            pts1_virt[idx] = [x, y, 1]
                            pts2_virt[idx] = [x, y, 1]
                    
                    self.pts1_virt.append(pts1_virt)
                    self.pts2_virt.append(pts2_virt)

    def _draw_matches(self, img1_path, img2_path, matches, output_path, max_matches=300):
        """Draw matches between two images with color-coded lines.
        
        Args:
            img1_path (str): Path to the first image
            img2_path (str): Path to the second image
            matches (np.ndarray): Array of matches, each row is [x1, y1, x2, y2]
            output_path (str): Path to save the visualization
            max_matches (int, optional): Maximum number of matches to draw. Defaults to 100.
        """
        try:
            # Read images
            img1 = cv2.imread(img1_path)
            img2 = cv2.imread(img2_path)
            
            if img1 is None or img2 is None:
                print(f"Warning: Could not read images at {img1_path} or {img2_path}")
                return
                
            # Limit number of matches to draw
            if len(matches) > max_matches:
                indices = np.random.choice(len(matches), max_matches, replace=False)
                matches = matches[indices]
            
            # Prepare image matching visualization
            h1, w1 = img1.shape[:2]
            h2, w2 = img2.shape[:2]
            
            # Create a black canvas for the images
            height = max(h1, h2)
            width = w1 + w2
            vis = np.zeros((height, width, 3), np.uint8)
            
            # Place images side by side
            vis[:h1, :w1] = img1
            vis[:h2, w1:w1+w2] = img2
            
            # Draw matches with color gradient
            num_matches = len(matches)
            for i, match in enumerate(matches):
                x1, y1, x2, y2 = match
                
                # Convert to integer coordinates
                pt1 = (int(x1), int(y1))
                pt2 = (int(x2) + w1, int(y2))
                
                # Generate a color based on the index (create a color gradient)
                # This will color code the lines from blue->green->red based on index
                color_idx = int(255 * i / num_matches)
                if color_idx < 85:
                    color = (255, 3 * color_idx, 0)  # Blue to green
                elif color_idx < 170:
                    color = (255 - 3 * (color_idx - 85), 255, 0)  # Green to red
                else:
                    color = (0, 255 - 3 * (color_idx - 170), 3 * (color_idx - 170))  # Red to blue
                
                # Draw the match
                cv2.circle(vis, pt1, 3, color, -1)
                cv2.circle(vis, pt2, 3, color, -1)
                cv2.line(vis, pt1, pt2, color, 1)
            
            # Add match count as text
            cv2.putText(vis, f"Matches: {num_matches}", (10, 30), 
                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
            
            # Save the visualization
            cv2.imwrite(f"{output_path}.jpg", vis)
            print(f"Match visualization saved to {output_path}.jpg")
            
        except Exception as e:
            print(f"Error drawing matches: {e}")

    def _load_features(self, feature_path):
        """Load features from the text file generated by gen_sideinfo.py.
        
        Args:
            feature_path (str): Path to the feature file
            
        Returns:
            tuple: (keypoints, descriptors, scales, orientations)
        """
        keypoints = []
        descriptors = []
        scales = []
        orientations = []
        
        with open(feature_path, 'r') as f:
            # Skip header line
            next(f)
            for line in f:
                parts = line.strip().split()
                if len(parts) >= 132:  # x, y, descriptor[128], scale, orientation
                    x = float(parts[0])
                    y = float(parts[1])
                    keypoints.append((x, y))
                    
                    # Get descriptor (elements 2-129)
                    descriptor = np.array([float(d) for d in parts[2:130]])
                    descriptors.append(descriptor)
                    
                    # Get scale and orientation
                    scale = float(parts[130])
                    orientation = float(parts[131])
                    scales.append(scale)
                    orientations.append(orientation)
        
        return np.array(keypoints), np.array(descriptors), np.array(scales), np.array(orientations)

    def _filter_one_to_one_matches(self, matches):
        """Filter matches to ensure one-to-one correspondence.
        
        Args:
            matches (list): List of matches from FLANN matcher
            
        Returns:
            list: Filtered matches ensuring one-to-one correspondence
        """
        # Create dictionaries to track matched points in both images
        query_indices = {}  # For image 1 (points in left image)
        train_indices = {}  # For image 2 (points in right image)
        
        # Sort matches by distance (better matches first)
        sorted_matches = sorted(matches, key=lambda x: x.distance)
        
        # Filter matches to ensure one-to-one correspondence
        filtered_matches = []
        for match in sorted_matches:
            query_idx = match.queryIdx
            train_idx = match.trainIdx
            
            # If both points haven't been matched yet, add this match
            if query_idx not in query_indices and train_idx not in train_indices:
                query_indices[query_idx] = train_idx
                train_indices[train_idx] = query_idx
                filtered_matches.append(match)
        
        return filtered_matches

    def __getitem__(self, index):
        """Get dataset sample.

        Args:
            index (int): sample index

        Returns:
            If mode is "train":
                tuple: points, side information, ground truth fundamental matrix, virtual points 1, virtual points 2
            If mode is "test":
                tuple: points, side information, ground truth rotation and translation, camera matrix
        """
        pts = self.pts[index]
        
        # add data if too small for training
        if self.num_points > 0 and pts.shape[0] < self.num_points:
            while pts.shape[0] < self.num_points:
                num_missing = self.num_points - pts.shape[0]
                idx = np.random.permutation(pts.shape[0])[:num_missing]

                pts_pert = pts[idx]
                pts = np.concatenate((pts, pts_pert), 0)

        # normalize side information
        side_info = pts[:, 4:] / np.maximum(np.amax(pts[:, 4:], 0), 1e-10)

        # Get point coordinates
        pts = pts[:, :4]

        # remove data if too big for training
        if self.num_points > 0 and (pts.shape[0] > self.num_points):
            idx = np.random.permutation(pts.shape[0])[: self.num_points]

            pts = pts[idx, :]
            side_info = side_info[idx]

        if self.mode == "train":
            # Return data for training
            F_gt = self.F_gt[index]
            pts1_virt = self.pts1_virt[index]
            pts2_virt = self.pts2_virt[index]
            return (np.float32(pts), side_info, np.float32(F_gt), np.float32(pts1_virt), np.float32(pts2_virt))
        else:
            # Return data for testing
            R_gt = self.R_gt[index]
            t_gt = self.t_gt[index]
            K = self.intrinsics[index]
            return (np.float32(pts), side_info, np.float32(R_gt), np.float32(t_gt), np.float32(K))
    
    def __len__(self):
        """Get length of dataset.

        Returns:
            int: length
        """
        return len(self.R_gt)


